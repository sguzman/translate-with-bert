./src/cleanup.rs
---
pub fn clean_text(raw: &str) -> Vec<String> {
    let mut paragraphs = vec![];

    for para in raw.split("\n\n") {
        let line = para.trim().replace("\n", " ");
        if !line.is_empty() {
            paragraphs.push(line.to_string());
        }
    }

    paragraphs
}


---
./src/io.rs
---
// src/io.rs

use anyhow::Result;
use colored::*;
use log::{debug, info};
use regex::Regex;
use std::{
    fs::File,
    io::{Read, Write},
    path::Path,
};

use crate::cleanup;

/// Split full text into paragraphs (cleaned).
pub fn split_into_paragraphs(text: &str) -> Vec<String> {
    cleanup::clean_text(text)
}

/// Split full text into sentences (keeping the punctuation).
pub fn split_into_sentences(text: &str) -> Vec<String> {
    let re = Regex::new(r"(?m)(.*?[\.\?!])\s+").unwrap();
    let mut v = Vec::new();
    let mut last = 0;
    for cap in re.captures_iter(text) {
        if let Some(m) = cap.get(1) {
            v.push(m.as_str().trim().to_string());
            last = m.end();
        }
    }
    if last < text.len() {
        v.push(text[last..].trim().to_string());
    }
    v.into_iter().filter(|s| !s.is_empty()).collect()
}

/// Write a chunk (lines) to disk.
pub fn write_chunk(path: &Path, lines: &[String]) -> Result<()> {
    let mut f = File::create(path)?;
    for line in lines {
        writeln!(f, "{}", line)?;
    }
    debug!(
        "üíæ Wrote {} lines to {}",
        lines.len().to_string().green(),
        path.display()
    );
    Ok(())
}

/// Merge `english.00.txt`‚Ä¶`english.NN.txt` into one file.
pub fn merge_chunks(cache_dir: &Path, out: &Path, total: usize) -> Result<()> {
    let mut f = File::create(out)?;
    for i in 0..total {
        let part = cache_dir.join(format!("english.{:02}.txt", i));
        if part.exists() {
            let mut buf = String::new();
            File::open(&part)?.read_to_string(&mut buf)?;
            writeln!(f, "{}\n", buf)?;
        }
    }
    info!(
        "üì¶ Merged {} chunks into {}",
        total.to_string().blue(),
        out.display()
    );
    Ok(())
}


---
./src/main.rs
---
// src/main.rs

mod cleanup;
mod io;
mod pipeline; // ensure cleanup is in scope

use anyhow::Result;

use env_logger::Env;
use indicatif::{ProgressBar, ProgressStyle};
use log::{error, info, warn};
use std::{fs, path::Path};
use tch::Device;

// Import colored extensions
use colored::Colorize;

fn main() -> Result<()> {
    // 0) Init logger
    env_logger::Builder::from_env(Env::default().default_filter_or("info")).init();

    // 1) Verify GPU availability
    let device = Device::cuda_if_available();
    match device {
        Device::Cuda(_) => info!("üñ•Ô∏è  Using device: {:?}", device),
        Device::Cpu => {
            error!(
                "‚ùå No CUDA GPU available (detected {:?}). Aborting.",
                device
            );
            std::process::exit(1);
        }
        _ => info!("üñ•Ô∏è  Using device: {:?}", device),
    }

    info!(
        "{}",
        "üöÄ Starting French‚ÜíEnglish translator (GPU batch)‚Ä¶".green()
    );

    // 2) Paths
    let input = Path::new("res/french.txt");
    let cache = Path::new(".cache");
    let output = Path::new("res/english.txt");

    fs::create_dir_all(cache)?;
    info!("üìÇ Cache dir ready: {}", cache.display());

    // 3) Read & split into paragraphs
    let text = fs::read_to_string(input)?;
    let mut paragraphs = io::split_into_paragraphs(&text);

    // 3a) For long paragraphs, break into sliding windows
    let max_sents = 32;
    let overlap = 8;
    let mut chunks = Vec::new();
    for para in paragraphs.drain(..) {
        let sents = io::split_into_sentences(&para);
        if sents.len() <= max_sents {
            chunks.push(para.clone());
        } else {
            let mut start = 0;
            while start < sents.len() {
                let end = usize::min(start + max_sents, sents.len());
                chunks.push(sents[start..end].join(" "));
                if end == sents.len() {
                    break;
                }
                start += max_sents - overlap;
            }
        }
    }

    let total = chunks.len();

    // 4) Progress bar
    let pb = ProgressBar::new(total as u64);
    pb.set_style(
        ProgressStyle::with_template(
            "{spinner:.green} {pos:>4}/{len:4} [{bar:40.cyan/blue}] {elapsed_precise}",
        )?
        .progress_chars("##-"),
    );

    // 5) Find missing chunks
    let mut missing_idx = Vec::new();
    for i in 0..total {
        let p = cache.join(format!("english.{:02}.txt", i));
        if p.exists() {
            pb.inc(1);
        } else {
            missing_idx.push(i);
        }
    }

    // 6) Configurable batch size (via BATCH_SIZE env, default 4)
    let batch_size = std::env::var("BATCH_SIZE")
        .ok()
        .and_then(|v| v.parse::<usize>().ok())
        .unwrap_or(4);
    info!(
        "üîÑ Translating {} missing chunk(s) in batches of {}‚Ä¶",
        missing_idx.len(),
        batch_size
    );

    // 7) Process in batches
    for batch in missing_idx.chunks(batch_size) {
        let batched_inputs: Vec<String> = batch.iter().map(|&i| chunks[i].clone()).collect();
        let now = std::time::Instant::now();
        let translations = pipeline::translate_chunks(&batched_inputs)?;
        let elapsed = now.elapsed();

        let timed = format!(
            "üî° Translated {} chunks in {} ms ({} seconds)",
            batch.len(),
            elapsed.as_millis(), // Corrected from as_millis() to as_millis()
            elapsed.as_secs()
        );
        warn!("{}", timed.yellow());

        for (j, &i) in batch.iter().enumerate() {
            let path = cache.join(format!("english.{:02}.txt", i));
            let translated_text = &translations[j];
            let lines: Vec<String> = translated_text.lines().map(str::to_string).collect();

            if let Err(e) = io::write_chunk(&path, &lines) {
                error!("‚ùå Failed writing chunk {:02}: {}", i, e);
            } else {
                info!("‚úÖ Saved chunk {:02}", i);
            }
            pb.inc(1);
        }
    }

    pb.finish_with_message("‚ú® All chunks processed");

    // 8) Merge into final file
    io::merge_chunks(cache, output, total)?;
    info!("üéâ Final output at {}", output.display());

    Ok(())
}


---
./src/pipeline.rs
---
// src/pipeline.rs

use anyhow::Result;
use log::{debug, info, error};
use rust_bert::pipelines::translation::{
    Language,
    // You can swap in a larger checkpoint like m2m100_1.2B
    TranslationModel,
};
use std::cell::RefCell;
use tch::Device;

use rust_bert::m2m_100::{
    M2M100ConfigResources, M2M100MergesResources, M2M100ModelResources, M2M100VocabResources
};
use rust_bert::pipelines::common::{ModelResource, ModelType};
use rust_bert::pipelines::translation::TranslationConfig;
use rust_bert::resources::RemoteResource;

thread_local! {
    /// One `TranslationModel` per thread.
    static THREAD_MODEL: RefCell<Option<TranslationModel>> = RefCell::new(None);
}

// Expect cuda to be available
pub fn cuda() -> Device {
    let device = Device::cuda_if_available();

    match device {
        Device::Cuda(_) => info!("üñ•Ô∏è  Using device: {:?}", device),
        Device::Cpu => {
            error!(
                "‚ùå No CUDA GPU available (detected {:?}). Aborting.",
                device
            );
            std::process::exit(1);
        }
        _ => info!("üñ•Ô∏è  Using device: {:?}", device),
    }
    device
}

pub fn build() -> TranslationModel {
    // 1. Weights & config for the XL (1.2 B) checkpoint
    let model_resource  = ModelResource::Torch(Box::new(RemoteResource::from_pretrained(
        M2M100ModelResources::M2M100_1_2B,
    )));
    let config_resource = RemoteResource::from_pretrained(
        M2M100ConfigResources::M2M100_1_2B,
    );

    // 2. ***This is the SentencePiece model, not vocab.json!***
    let vocab_resource  = RemoteResource::from_pretrained(
        M2M100VocabResources::M2M100_1_2B,   // points to `spiece.model`
    );

    let merges_resource = Some(RemoteResource::from_pretrained(
        M2M100MergesResources::M2M100_1_2B,   // merges.txt
    ));

    // 2) M2M-100 can translate between ANY pair of 100 languages,
    //    so we list all of them once for src and once for tgt.
    let source_languages = vec![Language::French]; // or .iter().collect()
    let target_languages = vec![Language::English];
    let device = cuda();

    // 3) Build a TranslationConfig **and** tweak repetition knobs
    let mut cfg = TranslationConfig::new(
        ModelType::M2M100,
        model_resource,
        config_resource,
        vocab_resource,
        merges_resource,
        source_languages,
        target_languages,
        device,
    );

    // ---- anti-repetition tweaks ---------------------------------
    cfg.no_repeat_ngram_size = 3; // forbid repeating any trigram
    cfg.repetition_penalty = 1.15;
    cfg.num_beams = 5; // higher-quality decoding
    //----------------------------------------------------------------

    // 4) Instantiate the pipeline
    TranslationModel::new(cfg)
        .unwrap()
}

/// Translate a batch of chunk-strings in one shot on the GPU.
/// Initializes the model once (on CUDA).
pub fn translate_chunks(inputs: &[String]) -> Result<Vec<String>> {
    debug!("üî° Translating batch of {} chunk(s)", inputs.len());
    THREAD_MODEL.with(|cell| -> Result<Vec<String>> {
        if cell.borrow().is_none() {
            
            let model = build();
            // Use the larger 1.2B variant for better fluency:
            *cell.borrow_mut() = Some(model);
        }
        let binding = cell.borrow();
        let model = binding.as_ref().unwrap();
        let outputs = model.translate(inputs, Some(Language::French), Some(Language::English))?;
        debug!("‚úÖ Batch translation completed");
        Ok(outputs)
    })
}


---
./src/test_gpu.rs
---
// src/bin/test_gpu.rs
use tch::Device;

fn main() {
    println!("Detected device: {:?}", Device::cuda_if_available());
}


---
